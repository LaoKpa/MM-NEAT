package edu.southwestern.networks.dl4j;

import static org.junit.Assert.assertArrayEquals;

import java.util.List;

import org.junit.Test;

import edu.southwestern.MMNEAT.MMNEAT;
import edu.southwestern.evolution.EvolutionaryHistory;
import edu.southwestern.evolution.genotypes.HyperNEATCPPNGenotype;
import edu.southwestern.evolution.genotypes.TWEANNGenotype;
import edu.southwestern.networks.ActivationFunctions;
import edu.southwestern.networks.TWEANN;
import edu.southwestern.networks.hyperneat.HyperNEATTask;
import edu.southwestern.networks.hyperneat.HyperNEATUtil;
import edu.southwestern.networks.hyperneat.Substrate;
import edu.southwestern.parameters.Parameters;
import edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask;
import edu.southwestern.util.random.RandomNumbers;

public class TensorNetworkFromHyperNEATSpecificationTest {

	@Test
	public void testFillWeightsFromHyperNEATNetwork() {
		HyperNEATTetrisTask.hardSubstrateReset();
		EvolutionaryHistory.archetypes = null; // Force reset
		MMNEAT.clearClasses();
		Parameters.initializeParameterCollections(new String[] {"runNumber:0","randomSeed:0","io:false","netio:false","maxGens:10","watch:false",
				"task:edu.southwestern.tasks.rlglue.tetris.HyperNEATTetrisTask",
				"rlGlueEnvironment:org.rlcommunity.environments.tetris.Tetris",
				"rlGlueExtractor:edu.southwestern.tasks.rlglue.featureextractors.tetris.RawTetrisStateExtractor",
				"rlGlueAgent:edu.southwestern.tasks.rlglue.tetris.TetrisAfterStateAgent",
				"splitRawTetrisInputs:true",
				"senseHolesDifferently:true",
				"hyperNEAT:true", // Prevents extra bias input
				"inputsUseID:true", // DL4J does this, so the HyperNEAT network must too to be compatible
				"linkExpressionThreshold:-0.1", // Express all links
				"stride:1","receptiveFieldSize:3","zeroPadding:false","convolutionWeightSharing:true",
				"HNProcessDepth:4","HNProcessWidth:4","convolution:true",
				"ftype:"+ActivationFunctions.FTYPE_RE_LU,
				"experiment:edu.southwestern.experiment.rl.EvaluateDL4JNetworkExperiment"});
		MMNEAT.loadClasses(); // Load parameters specified above
		
		HyperNEATTask hnt = HyperNEATUtil.getHyperNEATTask();		
		TensorNetworkFromHyperNEATSpecification tensorNetwork = new TensorNetworkFromHyperNEATSpecification(hnt);
		
        HyperNEATCPPNGenotype cppn = new HyperNEATCPPNGenotype();
        // Network generated by CPPN
        TWEANNGenotype substrateGenotype = cppn.getSubstrateGenotype(hnt);
        // DL4J network weights replaced with weights from HyperNEAT network
        tensorNetwork.fillWeightsFromHyperNEATNetwork(hnt, substrateGenotype);   
        // Wrap using my Network interface
		List<Substrate> substrates = hnt.getSubstrateInformation();
		int[] inputShape = HyperNEATUtil.getInputShape(substrates);
		int outputCount = HyperNEATUtil.getOutputCount(substrates);
		// DL4JNetworkWrapper implements Network
		DL4JNetworkWrapper dl4jNetwork = new DL4JNetworkWrapper(tensorNetwork, inputShape, outputCount);
        TWEANN substrateNetwork = substrateGenotype.getPhenotype();
    
		// Test 10 random inputs
		for(int i = 0; i < 10; i++) {
			// Should not be recurrent, but flush just in case
			substrateNetwork.flush();
			dl4jNetwork.flush();
			
	        double[] randomInput = RandomNumbers.randomArray(substrateGenotype.numIn);
	        double[] hyperNEATOutput = substrateNetwork.process(randomInput);
	        double[] dl4jOutput = dl4jNetwork.process(randomInput);

        // Debugging output
//        System.out.println("Inputs: " + Arrays.toString(randomInput));
//        for(Node n : substrateNetwork.nodes) {
//        	System.out.println(n.innovation + ":" + n.ftype + ":" + n.getActivation());
//        }
//        for(Layer lay : tensorNetwork.model.getLayers()) {
//        	System.out.println(lay.input());
//        }
//        
//        System.out.println("hyperNEATOutput: " + Arrays.toString(hyperNEATOutput));
//        System.out.println("dl4jOutput: " + Arrays.toString(dl4jOutput));

	        // The error margin here is fairly large. This is (presumably) because
	        // ND4J (I believe) favors float instead of double.
        	assertArrayEquals(hyperNEATOutput, dl4jOutput, 0.2);
		}
	}

}
